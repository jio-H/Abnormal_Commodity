{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import avg, count, countDistinct, max, min, mean, variance, stddev, sum, skewness, kurtosis\n",
    "from pyspark.sql.functions import hour,minute,second,year,month,dayofmonth,date_format\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n",
    "import csv\n",
    "from fitter import Fitter\n",
    "import os\n",
    "import shutil\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 22:10:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    appName(\"local[*]\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_struct = {}\n",
    "all_struct = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1ea2306d494098acf877405ee84638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13594a726c524b36a5335bc50370a87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dbe8d4a9ed4e60bdc456830595dc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abf806e67de42899ed641524cc1f887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7629ec46653e4e2182c0e80b5d479854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.6 s, sys: 4.28 s, total: 59.9 s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "@udf(returnType=StringType())\n",
    "def add_(a):\n",
    "    return str(a)+'$_$'\n",
    "for i in tqdm(range(6, 10)):\n",
    "    #遍历每一个月\n",
    "    path = \"../data_20210{}.tsv\".format(i)\n",
    "    df = spark.read.option(\"encoding\",\"gb18030\").csv(path,sep='\\t',header=True)\n",
    "    \n",
    "    #预处理\n",
    "    df =  df.filter(df['ITEM_SALES_VOLUME'] != 'null')\n",
    "    df = df.na.fill('Unkown')\n",
    "    df = df.withColumn('CATE_NAME_LV', F.concat(df['CATE_NAME_LV1'], df['CATE_NAME_LV2'], df['CATE_NAME_LV3'], df['CATE_NAME_LV4'],df['CATE_NAME_LV5']))\n",
    "    \n",
    "    #建表，保存数据，再一行一行处理\n",
    "    df.createOrReplaceTempView(\"data\")\n",
    "    if not os.path.exists('./P_{}'.format(i)):\n",
    "        tmp = spark.sql('select DATA_MONTH, ITEM_ID, ITEM_SALES_VOLUME, CATE_NAME_LV, ITEM_PRICE from data')\n",
    "        tmp.repartition(1).write.option(\"encoding\",\"gb18030\").csv('./P_{}'.format(i))\n",
    "    for file in os.listdir(\"./P_{}\".format(i)):\n",
    "        if file.endswith(\".csv\"):\n",
    "            with open(\"./P_{}/{}\".format(i, file), encoding='gb18030') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for tmp in tqdm(reader):\n",
    "                    if tmp[1] in st_struct:\n",
    "                        st_struct[tmp[1]][tmp[0]] = int(tmp[2])\n",
    "                    else:\n",
    "                        st_struct[tmp[1]] = {}\n",
    "                        st_struct[tmp[1]][tmp[0]] = int(tmp[2])\n",
    "    del(df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存id和月份信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_1 = {}\n",
    "ans = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e707bddf01f441e902406f5be21fbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7859140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = 0\n",
    "f = open('./falg1.csv', 'w', encoding='gb18030', newline='')\n",
    "writer = csv.writer(f)\n",
    "ff = open('./falg-1.csv', 'w', encoding='gb18030', newline='')\n",
    "writerf = csv.writer(ff)\n",
    "for i in tqdm(st_struct):\n",
    "    if(len(st_struct[i]) == 1):\n",
    "        st_1[i] = st_struct[i]\n",
    "        x = [i] + list(st_struct[i].keys())\n",
    "        writerf.writerow(x)\n",
    "    else:\n",
    "        try:\n",
    "            sum, tmp = np.sum(list(st_struct[i].values())), 1\n",
    "        except:\n",
    "            print(st_struct[i].values())\n",
    "        for j in st_struct[i]:\n",
    "            try:\n",
    "                avg = (sum - st_struct[i][j]) / (len(st_struct[i])-1)\n",
    "            except:\n",
    "                print(st_struct[i].values())\n",
    "            if 0 <= avg < 10:\n",
    "                tmp = 100\n",
    "            elif 10 <= avg < 100:\n",
    "                tmp = 50\n",
    "            elif 100 <= avg < 1000:\n",
    "                tmp = 10\n",
    "            elif 1000 <= avg < 100000:\n",
    "                tmp = 7\n",
    "            elif 10000 <= avg < 100000:\n",
    "                tmp = 5\n",
    "            else:\n",
    "                tmp = 2\n",
    "            if st_struct[i][j] > tmp * avg:\n",
    "                xx += 1\n",
    "                if i not in ans:\n",
    "                    ans[i] = {}\n",
    "                ans[str(i)][str(j)] = 1\n",
    "                writer.writerow([i, j])\n",
    "f.close()\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fb817978a040aab0ce4c8596ecf232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e775c29026d340d1ae9849b497386810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "st_1 = {}\n",
    "ans = {}\n",
    "with open(\"./falg-1.csv\", encoding='gb18030') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for tmp in tqdm(reader):\n",
    "        st_1[tmp[0]+'$_$'+tmp[1]] = 1\n",
    "\n",
    "with open(\"./falg1.csv\", encoding='gb18030') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for tmp in tqdm(reader):\n",
    "        ans[tmp[0]+'$_$'+tmp[1]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给原数据打上标记并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8545ff8e5ac4bdaa9288ed375243157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                        (0 + 4) / 14]\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "@udf(returnType=StringType())\n",
    "def add_flag(a):\n",
    "    if a in ans:\n",
    "        return '1'\n",
    "    elif a in st_1:\n",
    "        return '-1'\n",
    "    else:\n",
    "        return '0'\n",
    "for i in tqdm(range(6, 10)):\n",
    "    path = \"../data_20210{}.tsv\".format(i)\n",
    "    df = spark.read.option(\"encoding\",\"gb18030\").csv(path,sep='\\t',header=True)\n",
    "    df =  df.filter(df['ITEM_SALES_VOLUME'] != 'null')\n",
    "    df = df.na.fill('Unkown')\n",
    "    df = df.withColumn('CATE_NAME_LV', F.concat(df['CATE_NAME_LV1'], df['CATE_NAME_LV2'], df['CATE_NAME_LV3'], df['CATE_NAME_LV4'],df['CATE_NAME_LV5']))\n",
    "    df = df.withColumn(\"ITEM_ID\", add_(df[\"ITEM_ID\"]))\n",
    "    df = df.withColumn('tmp', F.concat('ITEM_ID', 'DATA_MONTH'))\n",
    "    df = df.withColumn(\"flag\", add_flag(df[\"tmp\"]))\n",
    "    df.createOrReplaceTempView(\"data\")\n",
    "    if not os.path.exists('./H_{}'.format(i)):\n",
    "        tmp = spark.sql('select * from data where flag = \"1\"')\n",
    "        tmp.repartition(1).write.option(\"encoding\",\"gb18030\").csv('./H_{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
