{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import avg, count, countDistinct, max, min, mean, variance, stddev, sum, skewness, kurtosis\n",
    "from pyspark.sql.functions import hour,minute,second,year,month,dayofmonth,date_format\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n",
    "import csv\n",
    "from fitter import Fitter\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 15:36:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    appName(\"local[*]\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solve:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        \"\"\"---------------------读入、去除价格为NULL、转换字段类型、建表、填充空值------------------------\"\"\"\n",
    "        \n",
    "        self.df06 = spark.read.option(\"encoding\",\"gb18030\").csv(path,sep='\\t',header=True)\n",
    "        self.df06 =  self.df06.filter(self.df06['ITEM_PRICE'] != 'null')\n",
    "        self.df06 = self.df06.withColumn(\"ITEM_PRICE\",self.df06.ITEM_PRICE.astype(\"int\"))\n",
    "        self.df06 = self.df06.withColumn(\"ITEM_SALES_VOLUME\",self.df06.ITEM_SALES_VOLUME.astype(\"int\"))\n",
    "        self.df06 = self.df06.withColumn(\"TOTAL_EVAL_NUM\",self.df06.TOTAL_EVAL_NUM.astype(\"int\"))\n",
    "        self.df06 = self.df06.withColumn(\"ITEM_STOCK\",self.df06.ITEM_STOCK.astype(\"int\"))\n",
    "        self.df06 = self.df06.withColumn(\"ITEM_FAV_NUM\",self.df06.ITEM_STOCK.astype(\"int\"))\n",
    "        self.df06.createOrReplaceTempView(\"data06\")\n",
    "        self.df06 = self.df06 = self.df06.na.fill('Unkown')\n",
    "        self.df06 = self.df06.withColumn('CATE_NAME_LV', F.concat(self.df06['CATE_NAME_LV1'],self.df06['CATE_NAME_LV2'],self.df06['CATE_NAME_LV3'],self.df06['CATE_NAME_LV4'],self.df06['CATE_NAME_LV5']))\n",
    "        \n",
    "        \"\"\"---------------------合并lv、添加INDEX（参数、得到文件路径------------------------\"\"\"\n",
    "        \n",
    "        @udf(returnType=StringType())\n",
    "        def replace_(a):\n",
    "            return str(a).replace(' ', '')\n",
    "        self.df06 = self.df06.withColumn(\"CATE_NAME_LV\", replace_(self.df06[\"CATE_NAME_LV\"]))\n",
    "        \n",
    "        @udf(returnType=StringType())\n",
    "        def plus_one(a):\n",
    "            tmp = str(a).replace(\"：\", \":\").replace(\"；\", \";\")\n",
    "            ans = ''\n",
    "            for i in tmp.split(\";\"):\n",
    "                ans += ' ' + i.split(\":\")[0]\n",
    "            return ans\n",
    "        self.df06 = self.df06.withColumn(\"INDEX\", plus_one(self.df06[\"ITEM_PARAM\"]))\n",
    "        self.df06 = self.df06.withColumn('C_I', F.concat(self.df06['CATE_NAME_LV'],self.df06['INDEX']))\n",
    "        self.path_l, self.path_n = os.path.split(path)\n",
    "        self.name, self.cs = os.path.splitext(self.path_n)\n",
    "        \n",
    "    def Cal_Li(self):\n",
    "        print(\"Cal_Li\")\n",
    "        \"\"\"---------------------将lv和price合并并保存------------------------\"\"\"\n",
    "        \n",
    "        @udf(returnType=StringType())\n",
    "        def add_(a):\n",
    "            return str(a)+'$_$'\n",
    "        self.df06 = self.df06.withColumn(\"C_P\", add_(self.df06[\"CATE_NAME_LV\"]))\n",
    "        \n",
    "        self.df06 = self.df06.withColumn('C_P', F.concat(self.df06['C_P'],self.df06['ITEM_PRICE']))\n",
    "        self.df06.createOrReplaceTempView(\"data06\")\n",
    "        tmp = spark.sql('select C_P from data06')\n",
    "        try:\n",
    "            tmp.repartition(1).write.option(\"encoding\",\"gb18030\").csv('{}/{}_C_P'.format(self.path_l, self.name))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \"\"\"---------------------保存的文件中读取并记录每个lv的每个商品价格(self.Li)------------------------\"\"\"\n",
    "        \n",
    "        cou_dir, can_dir = {}, {}\n",
    "        tmp_path = '{}/{}_C_P'.format(self.path_l, self.name)\n",
    "        def solve():\n",
    "            for i in os.listdir(tmp_path):\n",
    "                if os.path.splitext(os.path.join(tmp_path,i))[1] == '.csv':\n",
    "                    return os.path.join(tmp_path,i)\n",
    "        filepath = solve()\n",
    "        self.Li = {}\n",
    "        with open(filepath, encoding='gb18030') as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader)\n",
    "            for row in tqdm(reader):\n",
    "                tmp = row[0].split('$_$')\n",
    "                lv = tmp[0]\n",
    "                if len(tmp) < 2 or len(tmp) > 3:\n",
    "                    continue\n",
    "                if lv not in self.Li:\n",
    "                    self.Li[lv] = [int(tmp[1])]\n",
    "                else:\n",
    "                    self.Li[lv].append(int(tmp[1]))\n",
    "\n",
    "    def Deal_C_P(self):\n",
    "        print(\"Deal_C_P\")\n",
    "        ll_Thr = 2\n",
    "        \"\"\"---------------------计算没个lv的变异系数------------------------\"\"\"\n",
    "        \n",
    "        ll, ss, cc = {}, [], 0\n",
    "        cnt = 0\n",
    "        for i in self.Li:\n",
    "            if(np.mean(self.Li[i]) != 0):\n",
    "                x = np.std(self.Li[i])/np.mean(self.Li[i])\n",
    "                ll[i] = x\n",
    "            else:\n",
    "                ll[i] = 0\n",
    "            if(ll[i] < ll_Thr):\n",
    "                cnt += 1\n",
    "        print(cnt)\n",
    "        \"\"\"---------------------先对数正态分布拟合再正太分布拟合，并取出阈值------------------------\"\"\"\n",
    "        for i in tqdm(self.Li):\n",
    "            self.Li[i] = np.sort(self.Li[i])\n",
    "            \n",
    "        lv_can, lv_lr = {}, {}\n",
    "        for  i in tqdm((self.Li)):\n",
    "            if(ll[i] < ll_Thr):\n",
    "                continue\n",
    "            x = self.Li[i]\n",
    "            l = len(self.Li[i])//100\n",
    "            r = len(self.Li[i])*99//100+1\n",
    "            y = x[l: r]\n",
    "            f = Fitter(y, distributions=['lognorm'])\n",
    "            f.fit()\n",
    "            lv_can[i] = (f.fitted_param['lognorm'][1], f.fitted_param['lognorm'][2])\n",
    "            \n",
    "            yy = np.log((y-lv_can[i][0])/lv_can[i][1])\n",
    "            f = Fitter(yy, distributions=['norm'])\n",
    "            f.fit()\n",
    "            tmp = f.fitted_param['norm'][0] + 3*f.fitted_param['norm'][1]\n",
    "            lv_lr[i] = np.exp(tmp)*lv_can[i][1]+lv_can[i][0]\n",
    "            \n",
    "        \"\"\"---------------------根据阈值确定自定义字段flag1（异常）是否为1------------------------\"\"\"\n",
    "        @udf(returnType=StringType())\n",
    "        def cal_flag1(a):\n",
    "            tmp = str(a).split('$_$')\n",
    "            try:\n",
    "                if len(tmp) != 2:\n",
    "                    return str(1)\n",
    "                if(tmp[0] not in lv_lr):\n",
    "                    return str(0)\n",
    "                if(int(tmp[1]) > lv_lr[tmp[0]]):\n",
    "                    return str(1)\n",
    "                else:\n",
    "                    return str(0)\n",
    "            except:\n",
    "                return \"11\"\n",
    "        self.df06 = self.df06.withColumn(\"flag1\", cal_flag1(self.df06[\"C_P\"]))\n",
    "        \n",
    "         \n",
    "    def cal_Density(self):\n",
    "        print(\"cal_Density\")\n",
    "        p_rating = {}\n",
    "        pr_r = {}\n",
    "        ard = {}\n",
    "        M = 10\n",
    "        for i in tqdm(self.Li):\n",
    "            p_rating[i] = {}\n",
    "            if len(self.Li[i]) < M:\n",
    "                continue\n",
    "            N = len(self.Li[i])//10\n",
    "            l, r = np.min(self.Li[i]), np.max(self.Li[i])\n",
    "            cou = np.zeros(3*r+10)\n",
    "            fac = np.zeros(3*r+10)\n",
    "            faci = np.zeros(3*r+10)\n",
    "            sum_ans = np.zeros(3*r+10)\n",
    "            for j in self.Li[i]:\n",
    "                cou[j+r] += 1\n",
    "            for j in range(1, 3*r):\n",
    "                fac[j] = cou[j] + fac[j-1]\n",
    "                faci[j] = faci[j-1] + cou[j]*j\n",
    "            p_rating[i] = {}\n",
    "            for j in np.unique(self.Li[i]):\n",
    "                d = j + r\n",
    "                ll , rr = 0, r+1\n",
    "                while ll < rr:\n",
    "                    mid = (ll + rr) // 2\n",
    "                    sum = fac[d+mid] - fac[d-mid-1]-1\n",
    "                    if sum >= N:\n",
    "                        rr = mid\n",
    "                    else:\n",
    "                        ll = mid+1\n",
    "                sum = fac[d+ll] - fac[d-ll-1] - 1\n",
    "                sum_1 = fac[d+ll-1] - fac[d-ll] - 1\n",
    "                ans = 0\n",
    "                if sum == N:\n",
    "                    ans += faci[d+ll] - faci[d] - d * (fac[d+ll]-fac[d])\n",
    "                    ans += d * (fac[d-1]-fac[d-ll-1]) - (faci[d-1] - faci[d-ll-1])\n",
    "                else:\n",
    "                    ans += (N-sum_1) * (ll)\n",
    "                    ans += faci[d+ll-1] - faci[d] - d * (fac[d+ll-1]-fac[d])\n",
    "                    ans += d * (fac[d-1]-fac[d-ll]) - (faci[d-1] - faci[d-ll])\n",
    "                p_rating[i][j] = ans/N\n",
    "            \n",
    "        @udf(returnType=StringType())\n",
    "        def cal_rating(a):\n",
    "            tmp = a.split('$_$')\n",
    "            if tmp[0] in p_rating and int(tmp[1]) in p_rating[tmp[0]]:\n",
    "                return str(p_rating[tmp[0]][int(tmp[1])])\n",
    "            return '-1'\n",
    "        self.df06 = self.df06.withColumn(\"rating\", cal_rating(self.df06[\"C_P\"]))\n",
    "\n",
    "        lvmx_rating = {}\n",
    "        for i in p_rating:\n",
    "            for j in p_rating[i]:\n",
    "                lvmx_rating[i] = np.max([lvmx_rating.get(i, 0), int(p_rating[i][j])])\n",
    "\n",
    "        @udf(returnType=StringType())\n",
    "        def add_(a):\n",
    "            return str(a)+'$_$'\n",
    "        self.df06 = self.df06.withColumn(\"C_R\", add_(self.df06[\"CATE_NAME_LV\"]))\n",
    "        self.df06 = self.df06.withColumn('C_R', F.concat(self.df06['C_R'],self.df06['rating']))   \n",
    "        \n",
    "        @udf(returnType=StringType())\n",
    "        def cal_flag2(a):\n",
    "            tmp = a.split('$_$')\n",
    "            if tmp[0] not in lvmx_rating:\n",
    "                return '0'\n",
    "            else:\n",
    "                if len(str(int(lvmx_rating[tmp[0]]))) == len(tmp[1].split('.')[0]):\n",
    "                    return '1'\n",
    "                else:\n",
    "                    return '0'\n",
    "        self.df06 = self.df06.withColumn(\"flag2\", cal_flag2(self.df06[\"C_R\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cal_Li\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4487be818e4829bc2c8f299dead039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal_C_P\n",
      "9961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627a7302d66e4c1d86381b2d8e798749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daacfef51014a9db0877c655c6ed7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_Density\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c5cc373aa7445d876ce879ceb7d654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 20:57:04 WARN DAGScheduler: Broadcasting large task binary with size 1074.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 20:57:55 WARN DAGScheduler: Broadcasting large task binary with size 1114.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cal_Li\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65db90b83b184c09bf30a5fe343d6c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal_C_P\n",
      "10026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7c8bf57da64a9dbbc6d46d52c35e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aba29eea6b248e09c8e08ae40f64ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_Density\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3494821cb2364062ad14f39c604eb6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 21:04:32 WARN DAGScheduler: Broadcasting large task binary with size 1049.1 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 21:05:16 WARN DAGScheduler: Broadcasting large task binary with size 1089.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cal_Li\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8de5f608ad24e1295c93b4c0eaaf4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal_C_P\n",
      "10075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c607e103694fa5a72e45355c36a72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa067f2c95024f54be053afc21680b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_Density\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e088dc9192a043618da61920cb4a5619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 21:11:36 WARN DAGScheduler: Broadcasting large task binary with size 1061.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 21:12:23 WARN DAGScheduler: Broadcasting large task binary with size 1101.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cal_Li\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1daa0d4bcfb44c697757127b1024030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal_C_P\n",
      "9838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2997b4c0e46844c4971adf872a9b8a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfc57fdb42f4b2691787d05d4cf6784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_Density\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136cd4d62fd84fd8a894cfff1197f2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 21:21:54 WARN DAGScheduler: Broadcasting large task binary with size 1050.9 KiB\n",
      "22/04/15 21:22:41 WARN DAGScheduler: Broadcasting large task binary with size 1091.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:===================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37455\n",
      "CPU times: user 25min 32s, sys: 29.6 s, total: 26min 2s\n",
      "Wall time: 32min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sum = 0\n",
    "for i in range(6, 10):\n",
    "    path = '../data_20210{}.tsv'.format(i)\n",
    "    a = Solve(path)\n",
    "    a.Cal_Li()\n",
    "    a.Deal_C_P()\n",
    "    a.cal_Density()\n",
    "    a.df06.createOrReplaceTempView(\"data06\")\n",
    "    tmp = spark.sql('select *  from data06 where flag1 = \"1\" and flag2 == \"1\"')\n",
    "    x = tmp.count()\n",
    "    print(x)\n",
    "    sum += x\n",
    "    if not os.path.exists('./only_flag2_{}'.format(i)):\n",
    "        tmp.repartition(1).write.option(\"encoding\",\"gb18030\").csv('./only_flag2_{}'.format(i))\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
